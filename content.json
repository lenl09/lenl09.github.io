{"pages":[{"title":"What is the Onominute?","text":"Explaining the name itself.Tom Scott defines the ‘onosecond’ as: The second after you make a terrible mistake. The second when you realise what you just did and that there’s nothing you can do about it, the second when all the now-inevitable consequences flash through your mind. It is the second after you send a text message to the wrong person, after you delete the wrong file, after you spill the secret that you thought they already knew. The “onosecond” is that stomach-dropping, breath-stopping moment when all you can do is say “oh, no”. The Onominute (or 60 onoseconds) is a collection of my documentation and notes for learning various scientific topics. The name itself refers to the average number of times I realized my mistakes while learning a new topic, in a single sitting. Artist: Alex Norris Some stuff ‘bout me!As an aspiring computational scientist, I love to learn and read about mathematics, natural sciences, visual arts, design theory, and philosophy (especially metaphysics). My name is Nhan Luong (Luong Ha Tri Nhan) but most people know me as Len. I’m a Vietnamese student pursuing an Honours year in Computer Science at Monash University in Melbourne, Australia. Occasionally, I tend to just pick up my camera to go take random photos of cats or regular daily life stuff happening on the streets. I’m also trying to re-learn the piano, which I regrettably gave up on as a kid. My academic interests are scientific computing (specifically astronomy and physics), computer graphics (simulation and visualisation), and theoretical computer science. I’m hoping to be able to get involved in research projects in those areas! From now until July 2022, I will be focusing on my Honours thesis supervised by Professor Daniel Price and Dr. Max Cordeil, which is in interactive/immersive visualisation of particle-based fluid simulation for astrophysics. We will be visualising Prof. Daniel’s Smoothed Particle Hydrodynamics (SPH) simulations using the open-source IATK Unity3D toolkit developed by Dr. Max to interact with the fluid simulation in Virtual Reality. I don’t use IDE, not because I’m “hardcore”, but simply because I never learned to use them and I’m still too scared to do so (they look very intimidating!). My favorite text editor is Atom, I have been using vim for a bit to do my C classes but I mainly stick with Atom due to its massive community support, and because it looks aesthetically pleasing (I use a custom version of Atom Material for the UI and custom One Dark for syntax highlighting). What is this site about, then?This will be the main platform where I will be publishing the notes I have taken while trying to learn new subjects and topics to the masses. I want to write them in a way that is accessible and understandable by as many people as possilbe (meaning minimal prerequisite knowledge), not just because I want more people to discover and appreciate the beauty of science but also because it will vastly improve my understanding of the topic. As this is meant as a shared learning experience, not a go-to teaching material, there will definitely be errors and I welcome all the feedbacks and suggestions that you may have while reading my contents. Please feel free to email me at lennemo09@gmail.com if you ever notice a mistake! I will hopefully try to post updates on my projects if there are any, as I want to document my experience working on them as well, recording all the mistakes and challenges I might have faced so it can be helpful in the future for me or some lost soul Googling for the same problem.","link":"/about/"}],"posts":[{"title":"Hello World","text":"Cover photo by Michael DziedzicWelcome to the Onominute! This is basically my dictionary of useful templates for different plugins and functionalities I use for this blog. It also serves as a sandbox for testing other functionalities that I might want to add in the future. To truncate your post from being shown entirely on the front page, use this:1&lt;!-- more --&gt; References or FootnotesUsing hexo-footnotes plugin.Add the following tag to the post (Remove the \\’s before [‘s and ]’s):12345678910basic footnote\\[^1\\]here is an inline footnote\\[^2\\](inline footnote)and another one\\[^3\\]and another one\\[^4\\]\\[^1\\]: basic footnote content\\[^3\\]: paragraphfootnotecontent\\[^4\\]: footnote content with some [markdown](https://en.wikipedia.org/wiki/Markdown)Examples:Footnote number 1![1]Footnote number 2[2]and another one[3]and another one[4] Embedded Twitter postsUsing hexo-tag-twitter plugin.Add the following tag to the post:1{% twitter tweet-url %}Examples: Embedded Instagram postsUsing hexo-tag-instagram plugin.Add the following tag to the post:1{% instagram post-url %}or1{% instagram shortcode %}Examples: Embedded Flickr postsUsing hexo-tag-flickr plugin.Add the following tag to the post:1{% flickr [class1,class2,classN] photo_id [size] %}Example: Spoiler textUsing hexo-spoiler plugin.Add the following tag to the post:1{% spoiler option:value text... %}Example: Covered by box Covered by box Adding an image source buttonCover photo by Michael Dziedzic 1.basic footnote content ↩2.inline footnote ↩3.paragraph footnote content ↩4.footnote content with some markdown ↩","link":"/2021/08/12/hello-world/"},{"title":"The Elegant Mathematical Basis of Ray tracing - What Makes 3D Graphics Realistic","text":"This is an essay I submitted for a collection of texts about promoting the beauty of mathematics to the masses at Monash. It goes into the mathematics of one of the most important concepts in computer graphics, but at a very fundamental level. The reader should be able to follow this text with high-school level maths. SypnosisFor most of my friends, the natural world, they understandably believe, is impulsive, emotional, unpredictable, overall imperfect, and should be appreciated as such but that is not the case at all. Mathematics, aside from representing how humanity sees the world, helps us human beings specify the once general, afloat phenomena and things, and through that, figuring out underlying patterns in the random and use them to our advantage. It is thanks to those exact abstract numbers and formulas that reinvented ideas once only existing in the form of imaginative thoughts, such as spellbound princesses and talking toys into colorful reality right in front of our eyes. In fact, it is thanks to mathematics that a heartwarming stories such that of a clownfish crossing the ocean to find his son could have been encapsulated in flat screens and tiny electric boxes in the first place, completely transforming and enhancing our cinematic experience. Many ingenious computational techniques are responsible for the visual masterpieces mentioned above, one of which, a method called “ray tracing”, will be investigated in detail throughout this essay. Our light, the real kindIn order to simulate realistic lighting, we must first understand what is real light. We see the world around us by receiving light rays that bounce off of other objects to our eyes. This light is emitted by a light source, such as the sun or a lightbulb. The process seems to be rather simple so it must be easy to recreate on a computer, mustn’t it? Yes and no. It is not difficult to create a system that naively imitates the real world on a computer but it is immensely expensive to do so computationally. A real light source continuously emits an infinite amount of light rays; each of the rays bounces a number of times before it is fully absorbed or reaches our eyes - a process that requires an incredible number of computations just to mimic a single light source whereas in a typical scene of a Pixar animation, there are usually dozens. A naive approach“But we can’t calculate an infinite number of rays!”, one might say, and that is correct! What we can do is to limit the number of rays but this leads to a dilemma: either we have enough rays to make our light realistic but takes a very long time to render, or a sufficiently fast render but incredibly unrealistic results. Furthermore, it is not likely that most of the rays emitted from the light source are going to reach our “eyes”, which means most of the number-crunching work would not be entirely useful to us. One simple solution, ray tracing A way to minimise the number of non-useful computations is to look at the problem backwards, or only taking into account the ones that we need: instead of starting our simulation from the light source, we start from our virtual eyes: the camera. A digital image is segmented into individual pixels and for each pixel, we cast out a ray to find what would appear on that pixel by finding the closest object with which the ray intersects. We now know the objects that would be appearing on the screen and their colors. To find out if our point of interest is in direct illumination or in shade, we use secondary rays that work in the same manner. This technique is called “ray tracing” because we are tracing rays around the scene. When we add secondary rays, we are tracing a “path” of rays and this extension is called, unsurprisingly, “path tracing”. A rather simple, yet powerful, mathematical intuitionPolygonsNow that we have gotten the gist of ray tracing, let’s find out how we can tell the computer to solve our problem. Every 3D object is made of quadrilaterals called polygons and each polygon can be triangulated into 2 triangles: Modelling our problemFinding out whether a ray is intersecting an object essentially means checking if the ray has intersected one of these triangles. Now let’s model our system into a math problem. First, we pick a position for our camera in a coordinate system and call that point C = (C_x,C_y,C_z). We then create a hypothetical image plane that represents a grid of the pixels that would eventually appear on the viewer’s screen. We pick a pixel P = (P_x,P_y,P_z) on our grid and cast a ray from C through P which intersects which a polygon at the point M. We can then represent our ray (R) using the 3-dimensional parametric form of the equation for the line going through C and P with the parameter t: R(t) = (1-t)C + tPWhich represents the following linear system: \\begin{cases} x(t) = (1-t)C_x + tP_x \\\\ y(t) = (1-t)C_y + tP_y \\\\ z(t) = (1-t)C_z + tP_z \\end{cases}Using this form we can easily write C as R(0) and P as R(1).For our polygon, we can write the implicit form of the plane equation going through our polygon: ax + by + cz + d = 0Finding MThe point M lies on the plane, we have: aM_x + bM_y + cM_z + d = 0We know M also lies on the ray, this corresponds to a value for t, t' which returns our values for the coordinates of M: \\begin{cases} x(t') = (1-t')C_x + t'P_x = M_x \\\\ y(t') = (1-t')C_y + t'P_y = M_y \\\\ z(t') = (1-t')C_z + t'P_z = M_z \\end{cases}We now have a linear system of 4 unknowns (t', M_x, My, Mz)\\ with 4 equations: \\begin{cases} aM_x + bM_y + cM_z + d = 0 \\\\ M_x = (1-t')C_x + t'P_x \\\\ M_y = (1-t')C_y + t'P_y \\\\ M_z = (1-t')C_z + t'P_z \\end{cases}We substitute our ray equations for M into the plane equation for M to achieve an equation with a single unknown t': a[(1-t')C_x + t'P_x] + b[(1-t')C_y + t'P_y] + c[(1-t')C_z + t'P_z] + d = 0A computer can solve this equation almost instantaneously for the value of t', afterward we can substitute t' into our first system to get the coordinates of M. We now have our point: M = (M_x,M_y,M_z). Checking for intersectionThe next part of our problem is to examine if M lies within or outside our polygon. This does not require any more complicated mathematical tools than what we have been using, in fact it is simply an application of weighted averages. Let us imagine our polygon as the triangle EFG. The point M can be written as a weighted average of E, F and G as so: M = \\frac{k_1 E + k_2 F + k_3 G}{k_1 + k_2 + k_3}We can understand this as the larger the weight is for a vertex, the further away M is from that vertex relative to the other vertices, e.g., if k_1 = k_2 = k_3 then M is the same distance away from all three vertices (M is the center (or centroid) of the triangle EFG). If one of the weights is negative, however, then M is outside the triangle which corresponds to the fact that our ray does not intersect this polygon. Currently, our equation seems quite messy being a fraction of variables but there is one key fact that we can use to our advantage: as the weights represent the proportional distance between the vertices, we can scale the weights uniformly and it would not change the position of M thus we can limit the weights such that k_1 + k_2 + k_3 = 1 and therefore we can remove the denominator from our equation giving us: M = k_1 E + k_2F + k_3GWe can solve for the weights using the system of equations with 3 unknowns and 3 equations (again, this task only requires elementary row operations so the computer can solve it swiftly): \\begin{cases} M_x = k_1 E_x + k_2 F_x + k_3 G_x, \\\\ M_y = k_1 E_y + k_2 F_y + k_3 G_y, \\\\ M_z = k_1 E_z + k_2 F_z + k_3 G_z \\end{cases}We now got our weights k_1, k_2, k_3 and if none of the weights is negative, we have found that our ray intersects a polygon! Putting it all togetherAlthough we have only managed to figure out what color of the object that would appear on the screen at a pixel is, for realistic lighting we would also want to know if it is in shade. To do this instead of only sending out a single ray from a pixel, we send out a path of rays with each new rays originating from the intersection of the previous. We can use the same math we have used above only now instead of our point C, we choose the origin of our next ray to be M and the direction of the ray would be determined by the surface of the object (this information is included in the object’s shaders.) We have found a path tracing process for one pixel, all we need to do is to repeat this process for every other pixels of our image and luckily computers are quite keen of repetitive number crunching tasks like this (modern graphics processors can even do billions at the same time!). For a large scale production such as a Pixar’s animation, it is standard to use millions of rays for a single pixel and a few dozens or even hundreds bounces for each rays in order to achieve near-realistic lighting. For a marvel that is Toy Story 4, it took 60 to 160 hours to render one frame (this would be impossible if we use the aforementioned naive method) and the movie itself contains a few hundred thousands frames. This resulted in lighting and optical effects that is almost indifferentiable from real camera works. That’s simply how it all began!When most people think of advanced technologies they usually think of complex high-level mathematics but in fact, a lot of beautiful and complex creations, both natural or man-made, can be broken down into simple and elegant mathematics that everyone had already learned in school. Throughout this essay, we have discovered that beneath all the layers of abstraction of computer-generated graphics lies a set of simple highschool algebra homeworks. ReferencesP. Rademacher, “Ray Tracing: Graphics for the Masses”, University of North Carolina, n.d. [Online]. Available: https://www.cs.unc.edu/~rademach/xroads-RT/RTarticle.html. [Accessed May. 14, 2020]. S. Marschner, “Ray tracing”, lecture notes for CS 465 Computer Graphics I, Cornell University, Oct.10, 2003. [Online]. Available: http://www.cs.cornell.edu/courses/cs465/2003fa/lectures/notes-09raytracing.pdf. [Accessed May. 14, 2020]. K. Power. “Ray Tracing”, Institute of Technology, Carlow, Apr. 4, 2008. [Online]. Available: http://glasnost.itcarlow.ie/~powerk/GeneralGraphicsNotes/raytracing.html. [Accessed May. 15, 2020]. GeoGebra 3D, GeoGebra, 2020. [Online]. Available: https://www.geogebra.org/3d. [Accessed May. 14, 2020]. K. Morley, “NVIDIA OptiX Ray Tracing Powered by RTX”, NVIDIA Developer, Apr. 2, 2018. [Online]. Available: https://devblogs.nvidia.com/nvidia-optix-ray-tracing-powered-rtx/. [Accessed May. 15, 2020]. K. Desiderio and I.Phillips, “How Pixar’s animation has evolved over 24 years, from ‘Toy Story’ to ‘Toy Story 4’”, Insider, Jun. 21, 2019. [Online]. Available: https://www.insider.com/pixars-animation-evolved-toy-story-2019-6. [Accessed May. 16, 2020]. Walt Disney Animation Studios, “Disney’s Practical Guide to Path Tracing”, Aug. 9, 2016. [Video]. Available: https://www.youtube.com/watch?v=frLwRLS_ZR0. [Accessed May. 15, 2020]. S. Fong, “Rendering 101”, Khan Academy, Available: https://www.khanacademy.org/partner-content/pixar. [Accessed May. 15, 2020].","link":"/2021/08/13/Ray-tracing/"},{"title":"Making a Gaussian Transparent Shader in Unity","text":"&lt;/head&gt;During the very early stages of my thesis project, I was working on having the individual particles data visualised in Unity with IATK. Having the points show up is trivial, but having them rendered in a way that is visually meaningful is a bit more complicated. Eventually, I would have the particles being rendered as a continuos surface with the proper SPH kernel but just to get started, my supervisor suggested we implement a simple Gaussian alpha shader to each particle using its own smoothing length h. In this post, I will be discussing how I implemented the following Gaussian function to determine the alpha channel of a surface in Unity: f(x,y) = e^{-\\frac{(x-b)^2 + (y-b)^2}{2 \\cdot h^2}} What is a ‘Gaussian’?This is a super brief introduction of what a Gaussian is, I will have a seperate article on the in-depth mathematics of Gaussian functions later!.It can be confusing at first when you hear people referring to the term a Gaussian but essentially, it is simply a function whose plot is bell-shaped (or more correctly, spreading its parameters in a bell-shaped distribution). Usually I like to think of a Gaussian as some variation of the form: f(x) = a \\cdot e^{-\\frac{(x-b)^2}{2 \\cdot c^2}} = a \\cdot \\exp(-\\frac{(x-b)^2}{2 \\cdot c^2})For the above representation of the Gaussian, a is thought of as the height of the peak for our bell curve, b will determine the center of our bell curve (how the peak’s position it will be translated from the origin) and c will give us how wide the bell curve will be.Those who are more familiar with statistics or data science are more likely to recognise it as the probability density function for a Normal distribution: p(x) = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{(x-\\mu)^2}{2 \\cdot \\sigma^2}} = \\frac{1}{2\\pi\\sigma^2}\\exp(-\\frac{(x-\\mu)^2}{2 \\cdot \\sigma^2})As the problem I will be tackling doesn’t concern complicated parameters, I will be using a simplified form of one commonly used in mathematics: \\alpha(x) = \\exp(-\\frac{(x-d)^2}{2 \\cdot h^2})Where the function \\alpha(x) is the value for the alpha channel of our surface shader, which will be in the range [0,1] and therefore our peak height is a=1. The center of the distribution is determined by our offset parameter d and the width of the bell curve is the smoothing length h. Introduction to Unity shadersShaders in Unity is stored in .shader files, which are written in a declarative language called ShaderLab[2]. Let’s start by creating a new Unity scene by right clicking on the Project window and select Create &gt; Scene, I will have mine named TestScene.Next, let’s create some geometry to put our shader on. I will choose a Quad, as it is what my particles will be rendered as (a 2-D circle stencil on a square polygon).Now we need to create our Material and Shader files with Create &gt; Material and Create &gt; Shader &gt; Unlit Shader (I’m using an unlit shader because my particles will not be affected by lighting).To use the Material we’ve just made on the geometry, simply drag and drop the Material file from the Project window onto the object in the Scene view. Then we still need to apply our shader to the material we’ve just created, do this by clicking on the Material file in the Project window, in the Inspector, click on the Shader drop down and type in the name of the shader you’ve just created.Now, the appearance of the geometry will have changed based on the type of shader script you’ve just created. For unlit shader, it will be a plain pure white surface. Let’s open the shader script by double-clicking on in the Project window. The script should have already filled in with a templated provided by Unity with the necessary components and structures. To understand what each of the sections do, please check out the documentation from Unity[3].The first piece of code we will be changing is the Properties. This part contains names and declarations of variables that we will be passing to the GPU from the Inspector, so convenient! I will be adding my b and h values as it will be used by the surface shader to calculate the alpha value. I will add a tint to set the surface color of the geometry as well.1234567891011121314# Before:Properties { _MainTex (&quot;Texture&quot;, 2D) = &quot;white&quot; {} }# After:Properties { _MainTex (&quot;Texture&quot;, 2D) = &quot;white&quot; {} _MainTint (&quot;Main Tint&quot;, Color) = (1,1,1,1) # Base color of the surface h (&quot;h - smoothing length&quot;, Range(0,1)) = 0.5 # Smoothing length h from 0 to 1 b (&quot;b - center offset&quot;, Float) = 0.5 # Center of the Gaussian }Note that is is good practice to keep the same naming conventions for these variables (_CapitalCamelCase) while writing your code, I’m keeping mine as h and b for the sake of testing (please don’t do it)! Let’s go back to the editor and see what have changed. We can see that our newly added variables can be changed conveniently in the Inspector itself. The variable type[4] also reflects how the selection is presented in the Inspector GUI (i.e. Range will have a slider and Color will have a color selector). Working with the GPUWhat we have just done is we have informed Unity of what to provide to the GPU for calculation, but the GPU still has no idea of what those variables are or if they even exist, thus we have to declare them properly in SubShader‘s Pass section of the code as well, as this is what will be executed by the GPU. Right before the vert function (v2f vert (appdata v)) line, we can declare our variables here together with the existing _MainTex variable. The names must match what you have put in the Properties as Unity will be matching the names’ strings value to deliver their corresponding data.12345678910111213141516171819202122232425262728293031323334# Before:SubShader { ... ... Pass { ... ... sampler2D _MainTex; float4 _MainTex_ST; v2f vert (appdata v) ... ...# After:SubShader { ... ... Pass { ... ... sampler2D _MainTex; float4 _MainTex_ST; fixed4 _MainTint; float h; fixed b; v2f vert (appdata v) ... ...The types[5] of these variables might be unfamiliar to you but basically, float4 is a vector (or you can imagine it as an array) of 4 float values, so as fixed4 - which is a vector of 4 fixed values (Lowest precision fixed point value. Generally 11 bits, with a range of –2.0 to +2.0 and 1/256th precision). Notice that our Color variable is a fixed4, this is because Color is essentially a vector representing 4 channels of RGBA with each channel having a value clamped to the range [0,1]. Time to implement the mathNow, with the data ready to be used, we can finally do some math! For the purpose of this guide, we will be sticking with the Fragment shader and skip the Vertex shader. In short, the Fragment shader is the last stage (not really, but for now it can be intepreted as such) of the rendering pipeline, which determines how the pixels will be displayed on your screen. The Fragment shader code is used to interpolate between the vertices of your polygon and decide how they will appear on the screen, while the Vertex shader can alter the vertices, changing the how you see the geometry of the polygon (but not the actual polygon itself). Our target function here is the fixed4 frag (v2f i) : SV_Target function near the end of the file. Currently it is not very exciting:12345678fixed4 frag (v2f i) : SV_Target { // sample the texture fixed4 col = tex2D(_MainTex, i.uv); // apply fog UNITY_APPLY_FOG(i.fogCoord, col); # We can remove this as we're not using fog. return col; }Let’s try to see what it is doing. First it looks like it is creating a vector of 4 fixed values and returning it. That’s actually the color of the pixel that will be displayed on the screen! Meaning right here, right now, you have the full power over what every single pixel of that object will appear on the screen! Now that’s a magical feeling!The tex2D function seems like it is doing something with the _MainTex which is the base texture that was included with the script template, I’m guessing it is trying to set the pixel’s color to be matching the texture’s color in the corresponding position. But we’re not using a texture so let’s ignore that line and do some investigation! If I change the red channel of the col vector to 1, the quad should now be red:123456fixed4 frag (v2f i) : SV_Target { fixed4 col = tex2D(_MainTex, i.uv); col.r = 1; # You can refer to the channels with 'r','g','b' and 'a'. return col; }Hmmmm… That was dumb, of course it is still white, as white means all the channels are already 1! Let’s try again with the green and blue channels set to 0 and with it let’s do some thing to see what the i.uv part actually is:12345678910fixed4 frag (v2f i) : SV_Target { fixed4 col = tex2D(_MainTex, i.uv); if (i.uv[0] &lt; 0.5 &amp;&amp; i.uv[1] &lt; 0.5) { col.r = 1; col.b = 0; col.g = 0; } return col; }We have a red square on the bottom left quarted of the quad now! I might have cheated a little bit with some foresight by doing this once before, but now we can see that i.uv seems to be a vector or more specifically a point (x,y) on the geometry with their values seemingly also clamped from 0 to 1 with the origin being in the bottom left corner. Okay, now back to the real math. Let’s see what we have to do to get our Gaussian function implemented here. Recall we have the function: \\alpha(x) = \\exp(-\\frac{(x-d)^2}{2 \\cdot h^2})This is only for a 1-dimensional value but we need 2 values for x and y coordinates. To extend this to 2-dimensions, let’s look at where our calculation is dependent on the parameters. Only in (x-d)^2! This makes it much easier. Now that we are not only shifting on the x axis but also on the y axis as well, we need to extend this 1-D relation ship between our axis and our offset to 2-D by changing both of the terms to 2-D: x becomes a point (x,y) and d becomes a vector \\vec{d} = (d_x,d_y). We need to shift both of the dimensions similarly: \\alpha(x,y) = \\exp(-(\\frac{(x-d_x)^2}{2 \\cdot h^2}+\\frac{(y-d_y)^2}{2 \\cdot h^2})) = \\exp(-\\frac{(x-d_x)^2 + (y-d_y)^2}{2 \\cdot h^2})As we are shifting both dimensions equally by the amount b, we have the final equation: \\alpha(x,y) = \\exp(-\\frac{(x-b)^2 + (y-b)^2}{2 \\cdot h^2})Seems like we will have to do powers and exponential function in our calculation. Luckily, our script is using Cg - a shader programming language from NVIDIA, which comes packaged with super helpful math functions[6] such as pow() and exp()! We can now do the math in our shader code (notice I’ve set b to be 0.5 by default from the Properties section above):1234567891011fixed4 frag (v2f i) : SV_Target { fixed4 col = tex2D(_MainTex, i.uv)*_MainTint; # Applying the base color to our geometry. float x = i.uv[0]; # This is our x value float y = i.uv[1]; # This is our y value float value = exp(-(pow((x - b), 2) + pow((y - b), 2)) / (2 * pow(h, 2))); col.a = value; # Setting the alpha channel so it is more transparent the further away from the center. return col; } Investigating the shader transparencyThat’s very strange, seems like setting the alpha channel value is not doing what we have expected. Let’s try setting the color channels instead to see if our calculation is even working, and we already know setting the color values work.12345678910111213fixed4 frag (v2f i) : SV_Target { fixed4 col = tex2D(_MainTex, i.uv)*_MainTint; float x = i.uv[0]; float y = i.uv[1]; float value = exp(-(pow((x - b), 2) + pow((y - b), 2)) / (2 * pow(h, 2))); col.r = value; col.g = value; col.b = value; return col; }So it must not have been our math that was wrong (now that’s something you never hear). After some Googling, it appears we have to declare our ShaderLab Blend mode in order for the GPU to know how to combine the output of our fragment shader[7]. We do this by adding Blend SrcAlpha OneMinusSrcAlpha before our Pass block:12345678910...SubShader { Tags { &quot;RenderType&quot;=&quot;Opaque&quot; } LOD 100 Blend SrcAlpha OneMinusSrcAlpha Pass { ...It appears to be working but we can’t really see what’s going on with the skybox enabled. Disable this by clicking on this button on the top bar of the scene view:Now let’s see what we’ve got (The GIFs’ low bitrate compression made the color banding appear super ugly here, but it should be much nicer running on your computer):Below are some uncompressed screenshots of what the transparency should look like: We have a functioning Gaussian shader! Hopefully, with this basic introduction to Unity shaders and your creativity, you can make something much cooler than what I did here. Also, if you have noticed any errors or inefficiencies in what I have written, please let me know! I wrote this article while I was learning how to do it so I still have so much more to improve.(One thing I noticed is that doing the math for every single pixel on the screen is super expensive and as it is not dynamic, so I think should be precalculated and have the values of the Gaussian stored as a banded array). References1.Drawn with Desmos. ↩2.Unity's documentation on ShaderLab. ↩3.Unity's documentation on Custom shader fundamentals. ↩4.Unity's documentation on ShaderLab: defining material properties ↩5.Unity's documentation on Shader data types and precision ↩6.See Table E-1 on NVIDIA's Developer Zone: The Cg Tutorial - Appendix E ↩7.Unity's documentation on ShaderLab command: Blend ↩","link":"/2021/08/19/unity-gaussian-shader/"}],"tags":[{"name":"Sandbox","slug":"Sandbox","link":"/tags/Sandbox/"},{"name":"Testing","slug":"Testing","link":"/tags/Testing/"},{"name":"mathematics","slug":"mathematics","link":"/tags/mathematics/"},{"name":"computer graphics","slug":"computer-graphics","link":"/tags/computer-graphics/"},{"name":"3d","slug":"3d","link":"/tags/3d/"},{"name":"ray-tracing","slug":"ray-tracing","link":"/tags/ray-tracing/"},{"name":"unity","slug":"unity","link":"/tags/unity/"},{"name":"shaders","slug":"shaders","link":"/tags/shaders/"},{"name":"gaussian","slug":"gaussian","link":"/tags/gaussian/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"honours","slug":"honours","link":"/tags/honours/"}],"categories":[{"name":"Sandbox","slug":"Sandbox","link":"/categories/Sandbox/"},{"name":"Blog","slug":"Blog","link":"/categories/Blog/"},{"name":"Mathematics","slug":"Blog/Mathematics","link":"/categories/Blog/Mathematics/"},{"name":"Log","slug":"Log","link":"/categories/Log/"},{"name":"Honours","slug":"Log/Honours","link":"/categories/Log/Honours/"},{"name":"Computer Graphics","slug":"Log/Honours/Computer-Graphics","link":"/categories/Log/Honours/Computer-Graphics/"}]}